{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a514734-7628-4132-b4cb-25a683a07426",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats,special\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from random import choices\n",
    "from glob import glob\n",
    "import xarray as xr\n",
    "import datetime as dt\n",
    "from scipy.stats import multivariate_normal\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94fc2b1-c7d7-4a41-bb1d-8d08e203d4d1",
   "metadata": {},
   "source": [
    "# Probability Exercises Notebook\n",
    "#### **Author:** Sean Crowell\n",
    "#### **Date:** 06/14/2025\n",
    "#### **Purpose:** a limited set of exercises to refresh basic understanding of random variables, distributions, linear regression, and Bayes' Theorem. This should whet your appetite for the Summer School, where the concepts here will be treated as standard vocabulary. This notebook is roughly patterned after the excellent presentations by Dr. Michael Bertolacci, found here: XXX\n",
    "#### **License:** Free and open, but please acknowledge and keep this block when sharing. \n",
    "#### **Contact:** Please submit feedback to sean@belumenus.com. \n",
    "#### **External File:** airquality2.csv, which accompanies the R software package. More details here: https://www.rdocumentation.org/packages/datasets/versions/3.6.2/topics/airquality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53193502-942e-468e-bb8d-16ed2382d424",
   "metadata": {},
   "outputs": [],
   "source": [
    "airquality_file='./airquality2.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9cc065-7bf6-4d02-982e-7ce0fd6d2386",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "510c8661-6653-4faf-8ac7-47ae92bf83d5",
   "metadata": {},
   "source": [
    "## **Univariate Probability Distributions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1de773-1d02-4d3d-b451-aeeb54978eb6",
   "metadata": {},
   "source": [
    "#### A probability distribution $P$ is a model for our knowledge about one or more given variable(s) $X$ in the context that there are \"random\" variations in $X$ that we can't explain, and so we call $X$ a **random variable** as the value of $X$ may depend on unseen \"random\" factors. **Note that \"random\" doesn't mean \"without structure\"**. Most of the effort of an undergraduate probability and statistics course is directed towards characterizing different families of distributions to model the randomness we see in real datasets.\n",
    "\n",
    "#### One way of describing a distribution is its *Probability Density Function* (PDF) $p(x)$. The integral of the PDF, $\\int_a^b p(x) dx$, gives the probability that $X$ falls in a certain interval $[a,b]$. \n",
    "#### The *mean* is given by $$\\mu := E[x]:=\\int_A xp(x) dx$$ and *variance* by  $$\\sigma^2 := E[(x-\\mu)^2] := \\int_A (x-\\mu)^2 p(x) dx.$$ These numbers tell us about where the distribution is \"centered\" and how \"spread out\" it is. Sometimes we refer to square root of the variance, $\\sigma$, called the *standard deviation*.\n",
    "#### Technical note: the set $A$ in these integrals consists of all the values that make sense to evaluate $p(x)$, called the *support* of $p$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4716198-13f2-4d90-884c-28310287b138",
   "metadata": {},
   "source": [
    "### **Gaussian Distribution**\n",
    "#### The **Gaussian** (or Normal) distribution is the most important probability distribution. It is defined completely by its mean $\\mu$ and the variance $\\sigma^2$ and has PDF: $$p(x;\\mu,\\sigma^2) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^\\frac{-(x-\\mu)^2}{2\\sigma^2}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6334dc96-ad1c-493a-87f9-600c03c1ebe0",
   "metadata": {},
   "source": [
    "---\n",
    "This is just a convenience function so we don't have to rewrite the formula over and over.\n",
    "\n",
    "`mu` is the mean\n",
    "\n",
    "`sig` is the standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d575bd-b3d3-4368-8df1-9c3105a1b598",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaus_pdf(x,mu=0,sig=1):\n",
    "    return np.exp(-(x-mu)**2/sig**2/2)/sig/np.sqrt(2*np.pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0878b71c-10b7-4a79-aeb8-33c6a0c5be28",
   "metadata": {},
   "source": [
    "#### You can define a one variable function by specifying a single mean and standard deviation with the code \n",
    "#### `gpdf = lambda x: gaus_pdf(x,mu=mean_value,sig=stdev_value)`\n",
    "---\n",
    "### Example: Gaussian PDFs with different means and standard deviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fcdcdb-7c9e-4060-990e-a10b71cc732b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the mean and standard deviation:\n",
    "mu_0 = -2.0\n",
    "sig_0 = 0.5\n",
    "gpdf_0 = lambda x: gaus_pdf(x,mu=mu_0,sig=sig_0)\n",
    "# \n",
    "mu_1 = 1\n",
    "sig_1 = 2\n",
    "gpdf_1 = lambda x: gaus_pdf(x,mu=mu_1,sig=sig_1)\n",
    "\n",
    "x = np.linspace(-20,20,401)\n",
    "fig,ax = plt.subplots(1,1)\n",
    "g0 = ax.plot(x,gpdf_0(x))\n",
    "g1 = ax.plot(x,gpdf_1(x))\n",
    "ax.set_title('Univariate Gaussian PDFs')\n",
    "ax.legend([g0[0],g1[0]],[r'$\\mu = -2$, $\\sigma^2 = 1$',r'$\\mu = 0.5$, $\\sigma^2 = 4$']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97feeac-7ebf-402b-9bcf-3b8d4243a227",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "1. What happens to the PDF as we make `mu` larger or smaller?\n",
    "2. What happens to the PDF as we make `sig` larger or smaller?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053906c9-5c7e-4af9-a0fe-9439d8ddda27",
   "metadata": {},
   "source": [
    "#### We specified the mean `mu` and standard deviation `sig` using the convenience function, but does it match the definition above using the integral? The numpy function `trapz` is a handy simple way to compute an integral numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8c0963-a60d-43e6-bdd5-409ac2d186dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the mean and standard deviation with trapz for the first Gaussian PDF\n",
    "mu_s = np.trapz(x*gpdf_0(x),x)\n",
    "sig_s = np.sqrt(np.trapz((x-mu_0)**2*gpdf_0(x),x))\n",
    "\n",
    "print(f\"Specified Mean = {mu_0}, standard deviation = {sig_0}\")\n",
    "print(f\"Computed mean = {mu_s}, standard deviation = {sig_s}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd933fe-4a8c-495d-bd9c-509c44a4c7f8",
   "metadata": {},
   "source": [
    "#### Not bad! What about the second Gaussian that we called `gpdf_1`? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52e2f7a-e371-450b-8189-e96eb3b1cb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copy and paste the code above and change it to compute the mean and standard deviation for gpdf_1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab320923-6a26-408b-b20c-bb70d2e9fd35",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32380369-c0d3-439e-ae8e-ce5149405f14",
   "metadata": {},
   "source": [
    "### Sample Distributions vs. PDFs\n",
    "#### Observational data is usually a collection of samples. We can use simple tools like histograms to model the distribution of the samples with a theoretical distribution like a Gaussian.\n",
    "#### Simple example: The air quality dataset referenced in the notes gives us a good opportunity to explore different distributions. The reference is here: https://www.rdocumentation.org/packages/datasets/versions/3.6.2/topics/airquality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8334eb09-ddd2-473d-b18c-a0beb79f065d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_air_quality_data(file=None):\n",
    "    if file is not None:\n",
    "        df = pd.read_csv(file)\n",
    "        dates = np.array([dt.datetime(1973,df['Month'].iloc[i],df['Day'].iloc[i]) for i in range(len(df['Month']))])\n",
    "        df.index = dates\n",
    "        df['logOzone'] = np.log(df['Ozone'])\n",
    "        del df['Ozone']\n",
    "        del df['Month']\n",
    "        del df['Day']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1183506-8923-491c-ae39-110e64d61abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_air_quality_data(file=airquality_file)\n",
    "mu = df.mean()\n",
    "sig = df.std()\n",
    "fig,axs = plt.subplots(2,2,figsize=(12,12))\n",
    "for iky,ky in enumerate(sorted(df.keys())):\n",
    "    ax = axs[iky//2,iky%2]\n",
    "    gpdf = lambda x: gaus_pdf(x,mu[ky],sig[ky])\n",
    "    df[ky].hist(ax=ax)\n",
    "    xl = ax.get_xlim()\n",
    "    yl = ax.get_ylim()\n",
    "    x = np.linspace(xl[0],xl[1],21)\n",
    "    ax1 = ax.twinx()\n",
    "    ax1.plot(x,gpdf(x),color='tab:orange')\n",
    "    ax.set_title(ky)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62891d0a-43f0-4cb8-bfe7-cba37398db0e",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "1. Qualitatively, how well does the Gaussian density curves match the histograms?\n",
    "2. Qualitatively, how well do the mean and standard deviation describe the distribution of Solar Radiation, Temperature, Wind Speed, and logOzone during this time period?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e97c24-be5e-456a-8467-2d1c7fef4a69",
   "metadata": {},
   "source": [
    "## **Ordinary Least Squares (OLS) Linear Regression**\n",
    "#### Observational scientists often hypothesize about the relationships between different measured quantities using statistical models. The simplest such model is a 1-D linear function: $$y = \\alpha x + \\beta + \\epsilon.$$ Here $y$ is the observed variable we want to predict, $\\alpha$ and $\\beta$ are the slope and intercept parameters of the linear function, and $\\epsilon$ is a random variable that captures the assumed noise/error distribution in the observations and model. In the simplest case, we assume $\\epsilon\\sim N(0,\\sigma^2)$ for some variance $\\sigma^2$.\n",
    "#### If we have a collection of observations $y_1,y_2,...$ that go with inputs $x_1,x_2,...$, then we would like to choose $\\alpha$ and $\\beta$ so that the squared error between the predictions $\\hat{y}_i = \\alpha x_i + \\beta$ and the observations is minimized. $\\epsilon$ is ignored during this fitting process, but is a **critical** part of the assumptions for the OLS linear model to give trustworthy predictions. \n",
    "---\n",
    "### **Example: Ozone and Solar Radiation**\n",
    "#### It is well-understood that solar radiation and ozone production are related to one another. We want to see if there is a clear relationship in these data. \n",
    "#### Let's do some exploratory data analysis to see if a linear model would make sense. The first step is a simple scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f485d26-c77b-486a-bd2a-ec63849268ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_air_quality_data(file=airquality_file)\n",
    "fig,ax = plt.subplots(1)\n",
    "g1 = ax.scatter(df['Solar.R'],df['logOzone'])\n",
    "ax.set_title('LogOzone versus Solar Radiation')\n",
    "ax.set_ylabel('log(Ozone in ppb) ');\n",
    "ax.set_xlabel('Solar Radiation (langleys)');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537e2afa-9074-4161-8dd8-b794d1c45deb",
   "metadata": {},
   "source": [
    "#### The scatterplot \"looks like\" a line could pass nicely through the points and capture the upward trend, which suggests that we could use a linear function to predict FC temperatures from the ongoing temperature record in Denver.\n",
    "#### To find the optimal parameters $\\alpha$ and $\\beta$ in our linear model, we will use the Python library `statsmodels` (but we could  easily solve the basic matrix equations ourselves):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6138b595-6e84-4602-b566-ac830d01c2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "X = df['Solar.R']        # Predictors\n",
    "y = df['logOzone']       # Observations\n",
    "X = sm.add_constant(X)   # Allows for an offset\n",
    "ols = sm.OLS(y, X).fit() # Fit the model\n",
    "ols.summary()            # Print the results of the OLS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2723e31-e47c-436e-93f8-da0c18fc10d4",
   "metadata": {},
   "source": [
    "#### Whew! That's a lot of information. Let's summarize the important bits:\n",
    "1. The OLS model is logOzone = 2.7392 + 0.0038 Solar.R.\n",
    "2. The $R^2$ value, which is one measure of goodness-of-fit, is 0.180. This roughly means that this model explains 18% of the variability in the ozone observations (so something else must be causing much of the variability).\n",
    "3. The t-statistic tells us that the coefficients in our model can be trusted to be different from zero, i.e., that some degree of linear relationship exists.\n",
    "\n",
    "#### Let's plot the fitted line through the data and the histogram of the residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203ea8a4-22ba-43b0-ba91-1aad35f58806",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(1,2,figsize=(10,4))\n",
    "\n",
    "df['logOzone_ols'] = ols.fittedvalues\n",
    "ax = axs[0]\n",
    "df.plot.scatter(x='Solar.R',y='logOzone',ax=ax)\n",
    "df.plot(x='Solar.R',y='logOzone_ols',ax=ax,color='tab:orange')\n",
    "\n",
    "ax = axs[1]\n",
    "ols.resid.hist(ax=ax)\n",
    "#ax.legend(['OLS with Intercept','OLS without Intercept'])\n",
    "ax.set_xlabel('Observed - Predicted logOzone')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81b7557-2212-4080-af50-54866698112d",
   "metadata": {},
   "source": [
    "#### The predictions are on the left panel and the residuals (Observed - Predicted logOzone). \n",
    "---\n",
    "#### Questions:\n",
    "1. Does a linear model seem appropriate for this dataset?\n",
    "2. Do the residuals seem to be Gaussian distributions? What are the means and variances?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8deccfba-0dcc-43ce-ab75-579dbda3568a",
   "metadata": {},
   "source": [
    "## **Multivariate Probability Distributions**\n",
    "\n",
    "#### As our observations grow to multiple variables, we start to consider how our knowledge of those variables might be connected, where \"knowledge\" is a vague term that we typically use as shorthand for  probability distributions. \n",
    "---\n",
    "#### We saw in the last example that Solar Radiation and Ozone concentration seem to have a weak linear relationship. We can examine the other variables as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7c49ac-a9b2-42f6-8ab2-3ae7cde8611d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_air_quality_data(file=airquality_file)\n",
    "sns.pairplot(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca12cc0f-bdc5-46c8-9b02-fb4a9f06f16b",
   "metadata": {},
   "source": [
    "#### This matrix of plots shows the distributions of each variable as histograms along the diagonal and scatter plot of each pair of variables. The bottom row shows the ozone concentration as a function of the Solar Radiation, Wind Speed, and Temperature. \n",
    "\n",
    "#### Questions\n",
    "1. What does the pattern of dots tell us about the relationship between ozone and the other three variables?\n",
    "2. How about the other variables, e.g. temperature and wind speed?\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac16791-0593-458a-a081-3325730738d0",
   "metadata": {},
   "source": [
    "### **Joint Probability Distributions**\n",
    "#### We describe full state of knowledge of multiple variables as the **joint probability distribution**. We write (for two variables) as $Z = (X,Y)$ with distribution $P_Z$. As with the 1-D discussion above, we can discuss the **joint probability density function** $p(x,y)$ that we use in a similar fashion to compute probabilities, only now with double integrals. We also define the **marginal density function** for $x$: $$ p(x) = \\int p(x,y) dy $$ and similarly for $p(y)$.\n",
    "\n",
    "#### Generalizing other concepts from the 1-D case:\n",
    "#### The mean $\\mu = [\\mu_X,\\mu_Y]$ is just the variable-wise mean of the marginal density functions. \n",
    "#### The covariance between real-valued RVs $X$ and $Y$ is defined by\n",
    "#### $$cov(X,Y) = E[(X-\\mu_X)(Y-\\mu_Y)]$$ (now a double integral over x and y).\n",
    "#### Note that $cov(X,X) = \\sigma_X^2$ and $cov(X,Y) = cov(Y,X)$. The correlation between $X$ and $Y$ is covariance scaled by the individual standard deviations:\n",
    "#### $$\\rho(X,Y) = \\frac{cov(X,Y)}{\\sigma_X\\sigma_Y}.$$ Note that $\\rho(X,X) = 1$, $\\rho(X,Y) = \\rho(Y,X)$ and $-1 \\leq \\rho(X,Y) \\leq 1.$ We will often refer to the covariance matrix:\n",
    "#### $$ \\Sigma_{XY} = \\begin{pmatrix} \\sigma_X^2 & cov(X,Y) \\\\ cov(X,Y) & \\sigma_Y^2  \\end{pmatrix} $$ and the correlation matrix: $$ R_{XY} = \\begin{pmatrix} 1 & \\rho(X,Y) \\\\ \\rho(X,Y) & 1  \\end{pmatrix} $$\n",
    "\n",
    "---\n",
    "### **Air Quality Data Example:** \n",
    "#### Let's compute the covariance matrix and correlation for the variables in the air quality dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2cf5b8-daf9-42b1-894a-dcaf94e015d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_air_quality_data(file=airquality_file)\n",
    "\n",
    "mu = df.mean()\n",
    "cv = df.cov() #Pandas computes this easily\n",
    "cr = df.corr()\n",
    "fig,ax = plt.subplots(1)\n",
    "sns.heatmap(cr,ax=ax,cmap=plt.cm.RdBu_r,vmin=-1,vmax=1)\n",
    "xl = ax.get_xlim()\n",
    "yl = ax.get_ylim()\n",
    "for x in [1,2,3]:\n",
    "    ax.plot(np.array([x,x]),yl,'--k')\n",
    "    ax.plot(xl,np.array([x,x]),'--k')\n",
    "ax.set_title('Correlation Matrix for \\n Air Quality Dataset');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036c612c-a23c-4ba2-905f-0d90c9163a58",
   "metadata": {},
   "source": [
    "#### The blue colors indicate a negative correlation (anticorrelation), while red colors indicate a positive correlation. \n",
    "\n",
    "#### We note a few things: \n",
    "- Solar radiation and Temperature are correlated with ozone, while wind speed is anticorrelated with all the other variables. \n",
    "- The strongest correlation is between temperature and logOzone.\n",
    "- The strongest anticorrelation is between wind and logOzone.\n",
    "- Wind and temperature are also anticorrelated.\n",
    "\n",
    "#### Questions:\n",
    "1. Compare the correlation matrix with the scatter plots above. What does the negative correlation correspond to in the datasets?\n",
    "2. How about the positive correlation?\n",
    "3. What does the strength of the correlation correspond to?\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46360b68-2e0b-4463-aefc-954be29b9536",
   "metadata": {},
   "source": [
    "---\n",
    "### Multivariate Gaussian Distribution\n",
    "#### With these definitions in hand, we are ready to write down the joint density for the **multi-variate Gaussian** distribution with random vector $Z=(X,Y)$, mean vector $\\mu=(\\mu_X,\\mu_Y)$, and covariance matrix $\\Sigma$: $$ f(z; \\mu,\\Sigma)= (2\\pi)^{-1}|\\Sigma|^{-1/2}\\exp\\left(-\\frac{1}{2}(z - \\mu)^{T}\\Sigma^{-1}(z-\\mu)\\right)$$ where $|\\Sigma|$ is the determinant of the covariance matrix.\n",
    "#### **Important Notes**\n",
    "- The quantity $\\Sigma^{-1}(z-\\mu)$ is actually a matrix-vector product since $z$ and $\\mu$ are 2-D vectors. In fact, it will be a new column vector with two entries.\n",
    "- The entire quantity $(z - \\mu)^{T}\\Sigma^{-1}(z-\\mu)$ is a scalar, since $(z - \\mu)^{T}$ is a 1 x 2 row vector.\n",
    "- So even though we are talking about multivariate distributions, they are still predicting a number, i.e., the value of $Z$ is a real number."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7a2fb1-1a66-451e-8ef0-4e86e9370b8d",
   "metadata": {},
   "source": [
    "### 2-D Examples\n",
    "#### Let's use our favorite temperature dataset and examine the distributions for each pair of locations. We will plot 2D histograms as well as 2D Gaussian Densities with the same mean and covariance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef435e1e-7612-4231-87e4-52f425bb3dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_air_quality_data(file=airquality_file)\n",
    "\n",
    "#compute means and covariances\n",
    "mu = df.mean()\n",
    "cv = df.cov()\n",
    "\n",
    "fig,axs = plt.subplots(2,3,figsize=(16,8))\n",
    "for iky,ky in enumerate(['Solar.R','Temp','Wind']):\n",
    "    mu_s = np.array([mu[ky],mu['logOzone']])\n",
    "    cv_s = np.cov(df[ky],df['logOzone'])\n",
    "    x,y = np.meshgrid(np.linspace(df[ky].min(),df[ky].max(),21),np.linspace(df['logOzone'].min(),df['logOzone'].max(),21))\n",
    "    pos = np.dstack((x,y))\n",
    "    # Define the PDFs\n",
    "    pdf = multivariate_normal(mu_s,cv_s).pdf(pos)\n",
    "\n",
    "    ax = axs[0,iky]\n",
    "    g = ax.hist2d(df[ky],df['logOzone'],cmap=plt.cm.hot_r)\n",
    "    ax.set_title(f'2D Histogram of {ky} and logOzone ',fontsize=16);\n",
    "    ax.set_xlabel(ky);\n",
    "    ax.set_ylabel('logOzone');\n",
    "    plt.colorbar(g[-1],ax=ax)\n",
    "    \n",
    "    ax = axs[1,iky]\n",
    "    g = ax.contourf(x,y,pdf,cmap=plt.cm.hot_r)\n",
    "    ax.set_title(f'Gaussian Joint Density of \\n {ky} and logOzone',fontsize=16);\n",
    "    ax.set_xlabel(ky);\n",
    "    ax.set_ylabel('logOzone');\n",
    "    plt.colorbar(g,ax=ax)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb7d87d-b45b-45c5-bdd6-9e7dea8d6ba8",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "1. How well do the mean and covariance describe each of the sample joint distributions between logOzone and the other variables? If they do a good job, the sample histogram should look a lot like its Gaussian density. \n",
    "2. Would you use a Gaussian distribution as a model for the joint distribution of logOzone with any of the other variables? Which ones?\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0c1342-4236-426c-a07c-6d0f7a77d129",
   "metadata": {},
   "source": [
    "## **Multiple Linear Regression**\n",
    "#### We can generalize the OLS concepts to multiple predictors: $$y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_n x_n + \\epsilon.$$ Just like before, the $\\epsilon \\sim N(0,\\sigma^2)$ is a random variable that represents the error in modeling $y$ arising \"noise\" in the data. Observations are tuples $(\\hat{x}_1,...,\\hat{x}_n)$ with a coincident observed value $\\hat{y}$. We want to find a vector $\\beta = (\\beta_1,...\\beta_n)$ so that the predicted $y$ values are as close to the observations as possible. Practically this is very similar to the OLS problem  above, just with more unknowns. We solve a similar linear system of equations. \n",
    "---\n",
    "### Example: Predicting ozone with solar radiation, temperature, and wind\n",
    "#### Looking at the scatter plots above, we guess that a linear model could explain ozone variability in terms of the variability in the other variables. Let's use `statsmodels` again to derive a linear model. The high correlation between temperature and wind speed should give us pause - it can lead to multicollinearity as discussed in the lectures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ec4f0a-b3eb-4dd2-9947-2039153694dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "df = read_air_quality_data(file=airquality_file)\n",
    "X = np.column_stack((df['Solar.R'],df['Temp'],df['Wind']))        # Predictors\n",
    "y = df['logOzone']       # Observations\n",
    "ols = sm.OLS(y, X).fit() # Fit the model\n",
    "ols.summary()            # Print the results of the OLS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68147b54-7518-4c0b-a394-037c88b54401",
   "metadata": {},
   "source": [
    "Let's summarize the important bits:\n",
    "1. The OLS model is $$ \\mbox{logOzone} = 0.0022 \\mbox{Solar.R} + 0.0471 \\mbox{Temp} - 0.0642 \\mbox{Wind} $$\n",
    "2. The $R^2$ value, which is one measure of goodness-of-fit, is 0.983. That's significantly better than the OLS model with Solar.R alone.\n",
    "3. The t-statistics on the coefficients tell us that the coefficients in our model can be trusted to be different from zero, i.e., that some degree of linear relationship exists."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02eb609c-cedb-4b72-af60-8c96717bcffb",
   "metadata": {},
   "source": [
    "## Conditional Distributions\n",
    "#### In the setting of individual probability events $A$ and $B$, we define conditional probability as $$P(A|B) = \\frac{P(A\\cap B)}{P(B)}.$$ We interpret this as \"the probability A will occur assuming that B has occurred\". \n",
    "\n",
    "#### The conditional probability density $p(y|x)$ for two RVs $X$ and $Y$ can be defined similarly with the joint density $p(x,y)$ and the marginal density $p(x)$: $$ p(y|x) = \\frac{p(x,y)}{p(x)}.$$ Conceptually we think about this probability in similar terms, but of course computing probabilities requires integrating over the appropriate intervals of interest. \n",
    "\n",
    "#### This definition of $p(y|x)$ is not always straightforward to compute in practice, because it requires us to know the joint distribution. Luckily, we can derive a more straightforward tool for this purpose: Bayes' Theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bc7669-a5e6-44af-99af-0a9d8b0f7d06",
   "metadata": {},
   "source": [
    "### **Bayes' Theorem**\n",
    "#### If we repeat the conditional density formula for $p(x|y)$ and solve for $p(x,y)$, we get Bayes' Theorem: $$ p(x|y) = \\frac{p(y|x)p(x)}{p(y)} \\propto p(y|x)p(x)$$ We think about this equation as direction for using observations $y$ to enhance our understanding of a parameter $x$ from a previous state of knowledge that we call the **prior distribution** with density $p(x)$.  The distribution $p(y|x)$ is usually called the **likelihood**. The distribution $p(x|y)$ is called the **posterior distribution** in the context of updating our knowledge about the parameter $x$ using the prior and likelihood information.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de690ac4-1634-42b7-a620-89985033c197",
   "metadata": {},
   "source": [
    "## Bayesian Linear Models\n",
    "#### Following the lecture, we want to write $$ Y = X\\beta + \\epsilon, $$ where the X matrix is populated by the predictor variables, $\\beta$ is the vector of coefficients, and $\\epsilon \\sim N(0,\\Sigma_\\epsilon)$ is the observation uncertainty with mean 0 and covariance $\\Sigma_\\epsilon$.\n",
    "#### The Bayesian approach to this is to recast $\\beta$ as a random vector with some prior knowledge that is updated using observations $\\hat{y}$. If we assume that $\\beta \\sim N(\\mu_\\beta,\\Sigma_\\beta)$, we can write the exact form of the posterior distribution $$p(\\beta|Y) = \\frac{p(Y|\\beta)p(\\beta)}{p(Y)}.$$ The posterior distribution in this case is actually Gaussian as well, with covariance $$ \\widehat{\\Sigma}_\\beta =\\left( (X^\\mathsf{T}\\Sigma_{\\epsilon}^{-1}X)^{-1} + \\Sigma_{\\beta}^{-1}\\right)^{-1} $$ and mean $$ \\hat{\\beta} = \\widehat{\\Sigma}_\\beta \\left(X^\\mathsf{T}\\Sigma_{\\epsilon}^{-1}Y + \\Sigma_{\\beta}^{-1}\\mu_\\beta \\right). $$ This is an exact solution, so we can compute these directly if we have enough computing power and clever methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12b4c17-cb00-4ebb-9f08-82ef7423611b",
   "metadata": {},
   "source": [
    "---\n",
    "### Example: Air Quality Data\n",
    "#### Following the lecture, we seek a model form $$ \\mbox{logOzone} = \\beta_0 + \\beta_1 \\mbox{Solar.R} + \\beta_2 \\mbox{Wind} - \\beta_3 \\mbox{Temp}. $$ Note that we did this above with the OLS approach, meaning that we wanted **the $\\beta$** that minimize the squared errors in predictions versus our observations. This time we're going to follow a Bayesian approach. Hence we need to specify  \n",
    "1. the prior distribution with mean $\\mu_\\beta$ and covariance $\\Sigma_\\beta$\n",
    "2. the observational uncertainty distribution\n",
    "#### 1. We choose $\\mu_\\beta = (0,0,0,0)$ and $\\Sigma_\\beta = 10^2\\mathbf{I}.$This large variance (relative to the size of the OLS slopes we saw in the example above) means that we call these \"uninformative priors.\" The mean being 0 is somewhat of a \"null hypothesis\" that there is no deviation from zero - i.e., there is not a relationship. Also there is no correlation in the prior between the parameters.\n",
    "#### 2. We set $\\Sigma_\\epsilon = 0.467^2 \\mathbf{I}$ as is done in the lecture.\n",
    "#### Then just compute the matrix products using `np.dot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2d8903-86b6-4c5c-8eef-b8b47be66cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_air_quality_data(file=airquality_file)\n",
    "# Define the prior distribution\n",
    "prior_mean = np.zeros(4)\n",
    "prior_cov = 10**2*np.eye(4)\n",
    "# Observation error covariance\n",
    "obs_cov = 0.467**2*np.eye(len(df['logOzone']))\n",
    "\n",
    "#Precompute the matrix inverses\n",
    "prior_cov_inv = np.linalg.inv(prior_cov)\n",
    "obs_cov_inv = np.linalg.inv(obs_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81f12ca-4800-4419-9d3d-9a3008df5896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the matrix solution for the posterior covariance\n",
    "X = np.column_stack((np.ones(len(df['logOzone'])),df['Solar.R'],df['Wind'],df['Temp']))\n",
    "Xt_Sige_X = np.dot(X.T,np.dot(obs_cov_inv,X))\n",
    "post_cov = np.linalg.inv(Xt_Sige_X+prior_cov_inv)\n",
    "post_corr = np.zeros(post_cov.shape)\n",
    "for i in range(post_corr.shape[0]):\n",
    "    for j in range(post_corr.shape[1]):\n",
    "        post_corr[i,j] = post_cov[i,j]/np.sqrt(post_cov[i,i]*post_cov[j,j])\n",
    "\n",
    "#Compute the posterior mean\n",
    "Xt_Sige_Y = np.dot(X.T,np.dot(obs_cov_inv,df['logOzone']))\n",
    "Sigb_priormean = np.dot(prior_cov_inv,prior_mean)\n",
    "post_mean = np.dot(post_cov,Xt_Sige_Y + Sigb_priormean)\n",
    "print(f'Posterior mean (Const,Solar.R,Wind,Temp)={post_mean}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f43777b-330f-42bc-8af6-cdeb61e9d40a",
   "metadata": {},
   "source": [
    "#### How do the posterior mean values compare these values to the ones we got above with the OLS approach? With this Bayesian approach, we can also examine the correlations between these parameters. **Recall that there are no correlations in the prior covariance or observation covariance.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debb1d28-53f4-4b39-bbef-b18b234a5ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1)\n",
    "g = ax.imshow(post_corr,vmin=-1,vmax=1,cmap=plt.cm.RdBu_r)\n",
    "plt.colorbar(g,ax=ax)\n",
    "ax.set_xticks(range(4))\n",
    "ax.set_xticklabels(['Const','Solar.R','Wind','Temp'])\n",
    "ax.set_yticks(range(4))\n",
    "ax.set_yticklabels(['Const','Solar.R','Wind','Temp'])\n",
    "ax.set_title('Posterior Correlations in Linear Model Coefs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e4f004-ca5d-40c0-8c2d-27df3de2a23e",
   "metadata": {},
   "source": [
    "### Questions\n",
    "1. How would you interpret correlations in the posterior distribution for the parameters?\n",
    "2. Even though we assumed uncorrelated prior and observation error covariances, we still ended up with correlations in the posterior. Where were those introduced? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ec2bb6-a029-4ca1-b92e-fba579d28b41",
   "metadata": {},
   "source": [
    "---\n",
    "### **EXTRA CREDIT**: Observing System Error\n",
    "#### Suppose we are evaluating a new observing system for a quantity $y$ that we know well. We believe the observing system may be biased with error distribution $\\epsilon \\sim N(\\mu,\\sigma^2)$. Thus we take a bunch of measurements $y_1,y_2,...,y_n$ and examine the errors $\\epsilon_i = y-y_i$. If the number of samples is small and the variability among them seems large, a good Bayesian will incorporate a prior guess on $\\mu$ and $\\sigma^2$ and update it with the new information with Bayes' Rule. We need to create a prior distribution for these parameters as well as a likelihood. \n",
    "\n",
    "#### The typical prior model is a Gaussian model for the parameters. Suppose the manufacturer of our observing system tells us they think the mean error is 0.1 and the variance is 3. Being conservative, we decide that the uncertainty in their estimates is 100% as a one standard deviation value of their estimate. The Gaussian prior would then be $$p(\\mu,\\sigma) \\propto \\exp\\left(-\\frac{1}{2(0.1)^2}(\\mu-0.1)^2\\right)\\exp\\left(-\\frac{1}{2(3)^2}(\\sigma-3)^2\\right)$$ The natural model for the likelihood of the observed deviations $(\\epsilon_i)$ incorporating the parameters $x = (\\mu,\\sigma^2)$ is  given by the function $$ p((\\epsilon_i)|x) = (2\\pi\\sigma^2)^{-n/2}\\exp\\left(-\\frac{1}{2\\sigma^2}\\sum_{i=1}^{n}(\\epsilon_i-\\mu)^2\\right).$$ Note that the likelihood is a function of $\\mu$ and $\\sigma$ and is not in general a probability distribution for a specified set of observations $\\epsilon_i$.\n",
    "#### Our experiment is a \"simulation study\", meaning that we proceed as follows:\n",
    "1. Simulate some \"observations\" with a \"truth\" parameter set $(\\mu_t,\\sigma_t)$\n",
    "2. Construct a prior distribution with parameters $(\\mu_o,\\sigma_o)$\n",
    "3. Construct the likelihood function\n",
    "4. Multiply the likelihood and prior together and renormalize to get the posterior distribution.\n",
    "\n",
    "*Note*: we are solving this with a sampling approach instead of constructing the continuous version. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459f953d-8296-4d70-a96f-4b678b61c436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate some data\n",
    "mu = 1   #bias\n",
    "sig = 10 #error variability\n",
    "eps = mu + sig*np.random.randn(100)\n",
    "plt.hist(eps);\n",
    "plt.title('Sample Distribution of Observed Errors');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f67dd8-4a8c-43d8-bebf-f019f9a9fa04",
   "metadata": {},
   "source": [
    "#### The prior is the Gaussian with mean (0.1,3) and diagonal covariance matrix with diagonal values (0.1^2,9^2) due to our assumption of 100% uncertainty and independence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff9accb-f18c-444d-ad66-c359ac5a72a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's build a prior distribution for mu and sig\n",
    "mu_o = 0.1\n",
    "sig_o = 3\n",
    "\n",
    "# Sample grid for plotting\n",
    "mu_g = np.linspace(mu_o-3*sig_o,mu_o+3*sig_o,200)\n",
    "sig_g = np.linspace(0.0001,20,100)\n",
    "\n",
    "# Prior Distribution defined on sample grid\n",
    "prior = np.array([[gaus_pdf(mu_gg,mu_o,mu_o)*gaus_pdf(sig_gg,sig_o,sig_o) for sig_gg in sig_g] for mu_gg in mu_g])\n",
    "prior = prior/prior.mean()\n",
    "\n",
    "plt.pcolormesh(sig_g,mu_g,prior)\n",
    "plt.colorbar()\n",
    "plt.xlabel(r'$\\sigma$')\n",
    "plt.ylabel(r'$\\mu$')\n",
    "plt.title('Gaussian Prior Distribution $p(\\mu,\\sigma)$')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5f83f0-59bb-4666-a74b-e52926b5ce06",
   "metadata": {},
   "source": [
    "#### We can also plot the 2D Gaussian likelihood function $p((\\epsilon_i) | \\mu; \\sigma)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0463cf1-c470-4e83-96eb-f7040b55c8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaus_likelihood(eps,mu=0,sig=1):\n",
    "    if not isinstance(eps,(list,tuple,np.ndarray)): gl = gaus_pdf(eps,mu=mu,sig=sig)\n",
    "    gl = 1.\n",
    "    for ep in eps:\n",
    "        gl *= gaus_pdf(ep,mu=mu,sig=sig)\n",
    "    return gl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4939b0d-8164-4dde-ba9b-5d43cfd1ead4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample the likelihood function on a grid for plotting\n",
    "likelihood = np.array([[gaus_likelihood(eps,mu=mu_gi,sig=sig_gi) for sig_gi in sig_g] for mu_gi in mu_g])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6532e303-0ab1-44b9-b139-dc1209e96a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pcolormesh(sig_g,mu_g,likelihood)\n",
    "plt.colorbar()\n",
    "plt.title(\"Likelihood Function\",fontsize=18)\n",
    "plt.xlabel('$\\sigma$',fontsize=18)\n",
    "plt.ylabel('$\\mu$',fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4c833f-b520-4817-bb79-9ef67edad45b",
   "metadata": {},
   "source": [
    "#### In this setting, we call $p(\\mu,\\sigma|(\\epsilon_i))$ the posterior distribution since it is what we know after we take the observations $\\epsilon_i$ into account. Using Bayes' Rule above,the posterior density is proportional to the product of the likelihood $p((\\epsilon_i)|\\mu,\\sigma)$ and the prior $p(\\mu,\\sigma).$ To force $p(\\mu,\\sigma|(\\epsilon_i))$ to be a PDF, we can normalize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86937f02-e80f-47a2-8d61-7fcafcd00cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the posterior:\n",
    "posterior = likelihood*prior/np.nanmean(likelihood*prior) #this works because the grid is evenly spaced\n",
    "plt.pcolormesh(sig_g,mu_g,posterior)\n",
    "plt.colorbar()\n",
    "plt.xlabel(r'$\\sigma$')\n",
    "plt.ylabel(r'$\\mu$')\n",
    "plt.title('Posterior Distribution $p(\\mu,\\sigma|(\\epsilon_i))$');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41aa3948-eb6a-4714-84ba-de9b2e814601",
   "metadata": {},
   "source": [
    "### **Summary** \n",
    "- Our goal was to infer the observational error distribution $\\epsilon$.\n",
    "- We assumed the Gaussian model for the prior parameter distribution and likelihood function.\n",
    "- With the data we collected and Bayes' Theorem, we discovered a distribution for $\\mu$ and $\\sigma$ that incorporates the manufacturer's information and our measurements.\n",
    "\n",
    "#### Questions\n",
    "1. How is the posterior distribution different from the posterior distribution?\n",
    "2. What does the shape of the likelihood tell us?\n",
    "3. Does the manufacturer's specified error statistics match our updated information? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63437c39-86ef-4e05-94c2-b7de237685f1",
   "metadata": {},
   "source": [
    "### **Why a distribution for the parameters for the errors? Why not just values?**\n",
    "#### We recognize that we may have some knowledge on the parameters $\\mu$ and $\\sigma$ before even looking at the data, like the range of reasonable values that they fall into. The posterior density factors in both the mismatch to the data as well as this range of values and is a more complete description of the uncertainty than just single numbers. With a distribution, we can always find point estimates.\n",
    "#### The posterior mean parameter set is computed the normal way: $$ \\bar{\\mu} = \\int\\int mp(m,s|(\\epsilon_i))dsdm $$ $$ \\bar{\\sigma} = \\int\\int sp(m,s|(\\epsilon_i))dmds $$ We can also look at the mode or maximum of the posterior distribution and find the parameters that yield that value: $$ \\mu^*,\\sigma^* = \\mbox{argmax}\\: p(\\mu,\\sigma|(\\epsilon_i))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76312901-ca67-429c-8c51-8f9e6e6d3af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean of the posterior distribution with marginal distributions\n",
    "#mu_mean,sig_mean = np.nanmean(mu_g[:,None]*posterior),np.nanmean(sig_g[None]*posterior)\n",
    "# Mean and Mode of the posterior distribution with marginal distributions\n",
    "mu_mean,sig_mean = (mu_g*posterior.mean(1)).mean(),(sig_g*posterior.mean(0)).mean()\n",
    "mu_mode,sig_mode = mu_g[np.argmax((posterior.mean(1)))],sig_g[np.argmax(posterior.mean(0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca75165e-6bc4-4233-ac1d-5def2ab1788c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pcolormesh(sig_g,mu_g,posterior)\n",
    "g1 = plt.plot([sig_mode],[mu_mode],'r*')\n",
    "g2 = plt.plot([sig_mean],[mu_mean],'x',color='orange')\n",
    "plt.colorbar()\n",
    "plt.legend([g1[0],g2[0]],[f'Mode ($\\mu=${mu_mode:4.2f},$\\sigma=${sig_mode:4.2f})',f'Mean ($\\mu=${mu_mean:4.2f},$\\sigma=${sig_mean:4.2f})'])\n",
    "#plt.legend([g2[0]],[f'Mean ($\\mu=${mu_mean:4.2f},$\\sigma=${sig_mean:4.2f})'])\n",
    "plt.xlabel(r'$\\sigma$')\n",
    "plt.ylabel(r'$\\mu$')\n",
    "plt.title('Posterior Distribution $p(\\mu,\\sigma|(\\epsilon_i))$');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4f654b-a0e1-4bb3-8c78-d0920f1891cf",
   "metadata": {},
   "source": [
    "#### **Note**: in this problem the two estimates should be identical. The small differences arise from the sampling approach we took to solve the problem. If we represented the distributions from the start with continuous functions, the difference would disappear. \n",
    "\n",
    "#### Questions:\n",
    "1. How do these point estimates compare to what we assumed above?\n",
    "2. How would changing the assumptions about the prior uncertainties on $\\mu$ and $\\sigma$ change the results?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
